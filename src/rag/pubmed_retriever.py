import os, json, time, re
from typing import List, Dict, Tuple
import numpy as np
from tqdm import tqdm


# ===== ä¾èµ–æ£€æŸ¥ï¼ˆå‹å¥½æç¤ºï¼‰=====
try:
    from Bio import Entrez
except ImportError:
    raise ImportError("âŒ è¯·å…ˆå®‰è£…: pip install biopython")

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    raise ImportError("âŒ è¯·å…ˆå®‰è£…: pip install sentence-transformers")



class PubMedRetriever:
        
    def __init__(self, email: str = "xxx.com", cache_dir: str = "data/vectors"):
        """
        åˆå§‹åŒ–
        
        å‚æ•°:
            email: PubMed è¦æ±‚æä¾›é‚®ç®±ï¼ˆç”¨äºæµé‡è¿½è¸ªï¼Œä¸ä¼šæ»¥ç”¨ï¼‰
            cache_dir: å‘é‡ç¼“å­˜ç›®å½•
        """
        self.email = email
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆCPU å‹å¥½ï¼Œ70MBï¼‰
        print("åŠ è½½ Sentence-BERT æ¨¡å‹ (é¦–æ¬¡éœ€ä¸‹è½½ ~70MB)...")
        self.model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=cache_dir)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # ç”ŸåŒ–çŸ¥è¯†åº“ï¼šå¸¸è§é¶ç‚¹çš„ MeSH æœ¯è¯­æ˜ å°„
        self.target_mesh = {
            "egfr": '"Epidermal Growth Factor Receptor"[MeSH]',
            "her2": '"Receptor, ErbB-2"[MeSH]',
            "vegfr": '"Vascular Endothelial Growth Factor Receptor"[MeSH]',
            "pd-1": '"Programmed Cell Death 1 Receptor"[MeSH]',
            "ace2": '"Angiotensin-Converting Enzyme 2"[MeSH]',
        }
    
    def  _fetch_articles(self, query: str, max_results: int = 30) -> List[Dict]:
        """åº•å±‚ï¼šæ‰§è¡Œ PubMed æ£€ç´¢ + è§£æ"""
        print(f"ğŸ” PubMed æ£€ç´¢: {query[:70]}...")
        
        try:
            # æ£€ç´¢ PMID
            handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results, usehistory="y")
            results = Entrez.read(handle)
            id_list = results["IdList"]  # the structure of results?? # åŒ¹é…å’Œè·å–çš„ä¸ºä»€ä¹ˆè¿˜ä¸ä¸€æ ·ï¼Ÿï¼Ÿ
            print(f"åŒ¹é…ï¼š{results['Count']} , è·å–{len(id_list)}")
            if not id_list:
                return []
            # è·å–è¯¦æƒ…

            handle = Entrez.efetch(db='pubmed', id=id_list, rettype="abstract", retmode="xml")
            records = Entrez.read(handle)
            articles = []
            for record in records["PubmedArticle"]:
                try: 
                    medline = record["MedlineCitation"]
                    article = medline["Article"]
                    pmid = medline["PMID"]
                    title = article["ArticleTitle"]
                    abstract = article.get("Abstract", {}).get("AbstractText", [""])[0]
                    if isinstance(abstract, list):
                        abstract = " ".join(str(seg) for seg in abstract)
                    
                    if not abstract or len(abstract) < 50:
                        continue
                    articles.append(                        
                        {"pmid": str(pmid),
                        "title": title,
                        "abstract": abstract,
                        "journal": article["Journal"]["Title"],
                        "year": article["Journal"]["JournalIssue"]["PubDate"].get("Year", "N/A"),
                        "url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"})
                except Exception:
                    continue 
            print(f"ä¸€å…±æ‰¾åˆ°{len(articles)}ç¯‡æ–‡ç« ")

        except Exception as e:
            print(f"   âŒ PubMed é”™è¯¯: {type(e).__name__}: {e}")
            return []

    def _embed_and_cache(self, articles: List[Dict], cache_key: str) -> np.ndarray:
        """å‘é‡åŒ– + ç¼“å­˜"""
        if not articles:
            return np.array([])
        
        print(f"ğŸ§  å‘é‡åŒ– {len(articles)} ç¯‡æ‘˜è¦...")
        abstracts = [a["abstract"] for a in articles]
        embeddings = self.model.encode(abstracts, convert_to_numpy=True)
        
        # ä¿å­˜ç¼“å­˜
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump({
                "query": cache_key,
                "articles": articles,
                "embeddings": embeddings.tolist()
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç¼“å­˜: {cache_file}")
        
        return embeddings
    

    def _load_cache(self, cache_key: str) -> (List[Dict], np.ndarray):
        """åŠ è½½ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        if not os.path.exists(cache_file):
            return [], np.array([])
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data["articles"], np.array(data["embeddings"])
        except Exception:
            return [], np.array([])

        
       # ========== æ¨¡å¼ Aï¼šé¶ç‚¹é©±åŠ¨ï¼ˆAgent è‡ªåŠ¨è°ƒç”¨ï¼‰==========
    def retrieve_by_target(self, target: str, focus: str = "toxicity", top_k: int = 5) -> List[Dict]:
        """
        æŒ‰é¶ç‚¹+ç„¦ç‚¹æ£€ç´¢ï¼ˆé€‚åˆ Agent è‡ªåŠ¨å†³ç­–ï¼‰
        
        ç¤ºä¾‹:
            retriever.retrieve_by_target("EGFR", "cardiotoxicity")
            â†’ è‡ªåŠ¨ç”Ÿæˆ Query: "EGFR AND cardiotoxicity AND humans[MeSH]"
        """
        # æ„å»ºç¼“å­˜é”®ï¼ˆé¿å…é‡å¤è¯·æ±‚ï¼‰
        cache_key = f"{target.lower()}_{focus.lower()}"
        
        # å°è¯•åŠ è½½ç¼“å­˜
        articles, embeddings = self._load_cache(cache_key)
        if not articles:
            # ç”Ÿæˆä¸“ä¸š Queryï¼ˆè‡ªç”±æ–‡æœ¬ï¼Œé¿å… MeSH é™·é˜±ï¼‰
            query = f"{target} AND {focus} AND humans[MeSH]"
            articles = self._fetch_articles(query, max_results=30)
            if not articles:
                return []
            embeddings = self._embed_and_cache(articles, cache_key)
        
        # è¿”å›æœ€æ–°æ–‡çŒ®ï¼ˆæŒ‰å‘è¡¨æ—¶é—´éšå«æ’åºï¼‰
        return articles[:top_k]


    def retrieve_by_query(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æŒ‰è‡ªç„¶è¯­è¨€ Query æ£€ç´¢ï¼ˆé€‚åˆç”¨æˆ·æé—®ï¼‰
        
        ç¤ºä¾‹:
            retriever.retrieve_by_query("EGFR inhibitors QT prolongation arrhythmia")
            â†’ ç›´æ¥ç”¨è¯¥ Query æ£€ç´¢ + è¯­ä¹‰é‡æ’
        """
        # ç¼“å­˜é”® = Query å“ˆå¸Œï¼ˆç›¸åŒ Query å¤ç”¨ç»“æœï¼‰
        import hashlib
        cache_key = f"query_{hashlib.md5(query.encode()).hexdigest()[:8]}"
        
        # å°è¯•åŠ è½½ç¼“å­˜
        articles, embeddings = self._load_cache(cache_key)
        if not articles:
            # ç›´æ¥ç”¨ç”¨æˆ· Query æ£€ç´¢ï¼ˆä¸ä¿®æ”¹ï¼ï¼‰
            full_query = f"{query} AND humans[MeSH]"  # ä»…è¿½åŠ äººç±»ç ”ç©¶é™å®š
            articles = self._fetch_articles(full_query, max_results=30)
            if not articles:
                return []
            embeddings = self._embed_and_cache(articles, cache_key)
        
        # è¯­ä¹‰é‡æ’ï¼ˆç”¨æˆ· Query ä¸æ‘˜è¦è®¡ç®—ç›¸ä¼¼åº¦ï¼‰
        print(f"ğŸ¯ è¯­ä¹‰é‡æ’: '{query}'")
        query_emb = self.model.encode([query], convert_to_numpy=True)
        sims = np.dot(embeddings, query_emb.T).flatten()
        top_indices = np.argsort(sims)[::-1][:top_k]
        
        # é™„åŠ ç›¸ä¼¼åº¦åˆ†æ•°
        results = []
        for idx in top_indices:
            art = articles[idx].copy()
            art["similarity"] = float(sims[idx])
            results.append(art)
        
        return results
            



    
if __name__ == "__main__":
    EMAIL = "Xin.Xu1@etu.univ-grenoble-alpes.fr"  # â† â† â† æ›¿æ¢ä¸ºçœŸå®é‚®ç®±ï¼
    retriever = PubMedRetriever(email=EMAIL)
    
    # æµ‹è¯•1ï¼šæ¨¡å¼ A - é¶ç‚¹é©±åŠ¨ï¼ˆAgent è‡ªåŠ¨è°ƒç”¨ï¼‰
    print("="*70)
    print("ğŸ§ª æ¨¡å¼ A: é¶ç‚¹é©±åŠ¨æ£€ç´¢ (Agent ç”¨)")
    print("   åœºæ™¯: Agent å†³ç­–æ—¶è‡ªåŠ¨æŸ¥è¯¢ 'EGFR å¿ƒè„æ¯’æ€§'")
    print("="*70)
    results = retriever.retrieve_by_target("EGFR", "cardiotoxicity", top_k=2)
    for i, art in enumerate(results, 1):
        print(f"\n[{i}] PMID: {art['pmid']} | {art['year']} | {art['journal']}")
        print(f"    æ ‡é¢˜: {art['title'][:90]}...")
    
    # æµ‹è¯•2ï¼šæ¨¡å¼ B - ç”¨æˆ·é©±åŠ¨ï¼ˆè‡ªç„¶è¯­è¨€æé—®ï¼‰
    print("\n" + "="*70)
    print("ğŸ§ª æ¨¡å¼ B: ç”¨æˆ·é©±åŠ¨æ£€ç´¢ (äººæœºäº¤äº’ç”¨)")
    print("   åœºæ™¯: ç”¨æˆ·æé—® 'å“ªäº› EGFR æŠ‘åˆ¶å‰‚å¯¼è‡´ QT å»¶é•¿æˆ–å¿ƒå¾‹å¤±å¸¸ï¼Ÿ'")
    print("="*70)
    user_query = "EGFR inhibitors associated with QT prolongation or cardiac arrhythmia"
    results = retriever.retrieve_by_query(user_query, top_k=2)
    for i, art in enumerate(results, 1):
        print(f"\n[{i}] ç›¸ä¼¼åº¦: {art['similarity']:.3f} | PMID: {art['pmid']}")
        print(f"    æ ‡é¢˜: {art['title'][:90]}...")
        print(f"    æ‘˜è¦: {art['abstract'][:120]}...")
    
    print("\nâœ… åŒæ¨¡å¼æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ API ä½¿ç”¨æŒ‡å—:")
    print("   â€¢ Agent å†³ç­– â†’ ç”¨ retrieve_by_target(target, focus)")
    print("   â€¢ ç”¨æˆ·æé—®   â†’ ç”¨ retrieve_by_query(natural_language_query)")

